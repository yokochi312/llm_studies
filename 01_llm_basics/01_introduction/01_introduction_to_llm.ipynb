{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6fc4ed",
   "metadata": {},
   "source": [
    "# 大規模言語モデル（LLM）入門\n",
    "\n",
    "このノートブックでは、大規模言語モデル（Large Language Models, LLMs）の基本概念について学習します。\n",
    "\n",
    "## 目次\n",
    "1. LLMとは何か\n",
    "2. LLMの主要な特徴\n",
    "3. 代表的なLLMモデル\n",
    "4. LLMの応用分野"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e77b9",
   "metadata": {},
   "source": [
    "## 1. LLMとは何か\n",
    "\n",
    "大規模言語モデル（LLM）は、大量のテキストデータを学習し、人間のような自然言語処理能力を持つAIモデルです。\n",
    "これらのモデルは以下のような特徴を持っています：\n",
    "\n",
    "- 大規模なデータセットでの事前学習\n",
    "- Transformerアーキテクチャの活用\n",
    "- 文脈理解と生成能力\n",
    "- マルチタスク処理能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eead36",
   "metadata": {},
   "source": [
    "## 2. LLMの主要な特徴\n",
    "\n",
    "### スケール\n",
    "- パラメータ数：数十億から数兆\n",
    "- 学習データ：数百GB～数TB\n",
    "- 計算リソース：大規模なGPUクラスター\n",
    "\n",
    "### アーキテクチャ\n",
    "- Transformerベース\n",
    "- 自己注意機構\n",
    "- 深層学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1312b",
   "metadata": {},
   "source": [
    "## 3. 代表的なLLMモデル\n",
    "\n",
    "現在、多くのLLMが公開されています：\n",
    "\n",
    "- GPT（OpenAI）\n",
    "- Claude（Anthropic）\n",
    "- LLaMA（Meta）\n",
    "- PaLM（Google）\n",
    "- BERT（Google）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4fb3c",
   "metadata": {},
   "source": [
    "## 4. LLMの応用分野\n",
    "\n",
    "LLMは様々な分野で活用されています：\n",
    "\n",
    "- テキスト生成・要約\n",
    "- 質問応答\n",
    "- コード生成\n",
    "- 言語翻訳\n",
    "- 感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b9897-aa79-4e45-b219-2e35d29c28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(f\"Transformersバージョン: {transformers.__version__}\")\n",
    "print(f\"PyTorchバージョン: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb3ca2",
   "metadata": {},
   "source": [
    "## 5. 実践：Transformerモデルの基本操作\n",
    "\n",
    "ここからは、実際にTransformerモデルを使って実験を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 小規模なGPT-2モデルを使用します\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# トークナイザーとモデルの読み込み\n",
    "print(\"トークナイザーを読み込んでいます...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"\\nモデルを読み込んでいます...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"\\n準備完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ef04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト生成の例\n",
    "input_text = \"Artificial Intelligence is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# テキスト生成\n",
    "print(\"テキスト生成中...\")\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 生成されたテキストのデコード\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"\\n入力テキスト: {input_text}\")\n",
    "print(f\"生成されたテキスト: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da88601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークン化の実験\n",
    "example_text = \"これは自然言語処理の実験です。LLMについて学んでいきましょう！\"\n",
    "\n",
    "# テキストのトークン化\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "token_ids = tokenizer.encode(example_text)\n",
    "\n",
    "print(\"テキスト:\", example_text)\n",
    "print(\"\\nトークン:\", tokens)\n",
    "print(\"\\nトークンID:\", token_ids)\n",
    "\n",
    "# トークンの数を表示\n",
    "print(f\"\\nトークンの数: {len(tokens)}\")\n",
    "\n",
    "# トークンIDからテキストに戻す\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"\\n復元されたテキスト: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ceb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# より詳細なテキスト生成の実験\n",
    "def generate_text(prompt, max_length=50, num_sequences=3, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # テキスト生成\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_sequences,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 生成されたテキストの表示\n",
    "    print(f\"入力プロンプト: {prompt}\\n\")\n",
    "    print(\"生成されたテキスト:\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"\\n{i+1}番目の生成結果:\")\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 異なるプロンプトとパラメータで試してみる\n",
    "prompts = [\n",
    "    \"The future of AI is\",\n",
    "    \"In the next decade, technology will\",\n",
    "    \"Artificial Intelligence can help humans by\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"=\" * 80)\n",
    "    generate_text(prompt, max_length=70, num_sequences=3, temperature=0.8)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf62cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストの感情分析：\n",
      "--------------------------------------------------\n",
      "\n",
      "テキスト: I love working with artificial intelligence!\n",
      "感情: POSITIVE\n",
      "確信度: 0.999\n",
      "--------------------------------------------------\n",
      "\n",
      "テキスト: This technology is very complicated and frustrating.\n",
      "感情: NEGATIVE\n",
      "確信度: 0.998\n",
      "--------------------------------------------------\n",
      "\n",
      "テキスト: AI has both benefits and challenges to consider.\n",
      "感情: POSITIVE\n",
      "確信度: 0.999\n",
      "--------------------------------------------------\n",
      "\n",
      "テキスト: The future of AI looks promising and exciting!\n",
      "感情: POSITIVE\n",
      "確信度: 1.000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 感情分析のパイプラインを準備\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 分析するテキストの例\n",
    "texts = [\n",
    "    \"I love working with artificial intelligence!\",\n",
    "    \"This technology is very complicated and frustrating.\",\n",
    "    \"AI has both benefits and challenges to consider.\",\n",
    "    \"The future of AI looks promising and exciting!\"\n",
    "]\n",
    "\n",
    "print(\"テキストの感情分析：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 各テキストの感情を分析\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)\n",
    "    sentiment = result[0]\n",
    "    \n",
    "    print(f\"\\nテキスト: {text}\")\n",
    "    print(f\"感情: {sentiment['label']}\")\n",
    "    print(f\"確信度: {sentiment['score']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a12823b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e8b72e29a34c32a16168e14231c3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1eaeb787197411b8270fdb806bcf9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4a099881814fd690a217329e0f8645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1714674b955147df9a3affd7ab987266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a161f5429b74f47b7ffd2dd7e6aea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a9c9e93af473da5ef92a820a132c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のテキスト:\n",
      "--------------------------------------------------\n",
      "\n",
      "Artificial Intelligence has transformed the way we live and work in the 21st century. \n",
      "Machine learning algorithms are now used in healthcare to diagnose diseases, in finance \n",
      "to detect fraudulent transactions, and in transportation to develop self-driving cars. \n",
      "These AI systems process vast amounts of data to identify patterns and make predictions \n",
      "with increasing accuracy. However, the rapid advancement of AI technology also raises \n",
      "important ethical questions about privacy, bias, and the future of human employment. \n",
      "Researchers and developers are working to address these challenges while continuing to \n",
      "push the boundaries of what AI can achieve.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "短い要約:\n",
      "Artificial Intelligence has transformed the way we live and work in the 21st century. Machine learning algorithms are now used in healthcare to diagnose diseases, in finance to detect fraudulent transactions, and in transportation to develop self-driving cars. However\n",
      "\n",
      "やや長めの要約:\n",
      "Artificial Intelligence has transformed the way we live and work in the 21st century. Machine learning algorithms are now used in healthcare to diagnose diseases, in finance to detect fraudulent transactions, and in transportation to develop self-driving cars. However, the rapid advancement of AI technology also raises ethical questions about privacy, bias, and the future of human employment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 要約用のパイプラインを準備\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# 要約するテキスト（AIに関する長めの文章）\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence has transformed the way we live and work in the 21st century. \n",
    "Machine learning algorithms are now used in healthcare to diagnose diseases, in finance \n",
    "to detect fraudulent transactions, and in transportation to develop self-driving cars. \n",
    "These AI systems process vast amounts of data to identify patterns and make predictions \n",
    "with increasing accuracy. However, the rapid advancement of AI technology also raises \n",
    "important ethical questions about privacy, bias, and the future of human employment. \n",
    "Researchers and developers are working to address these challenges while continuing to \n",
    "push the boundaries of what AI can achieve.\n",
    "\"\"\"\n",
    "\n",
    "# テキストを要約\n",
    "print(\"元のテキスト:\")\n",
    "print(\"-\" * 50)\n",
    "print(long_text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 要約を生成（異なる長さで試してみる）\n",
    "print(\"\\n短い要約:\")\n",
    "short_summary = summarizer(long_text, max_length=50, min_length=10, do_sample=False)\n",
    "print(short_summary[0]['summary_text'])\n",
    "\n",
    "print(\"\\nやや長めの要約:\")\n",
    "long_summary = summarizer(long_text, max_length=100, min_length=30, do_sample=False)\n",
    "print(long_summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74889ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdc210093284f3b979ee77d8041f71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cb8cbe873b4faba7b66dde7944841e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1f882c67db4468b331508811e7ee21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39ddc9946d54060add6ec362ddec8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62a0bcad16c45d0b24d5390b92c1ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問応答テスト：\n",
      "--------------------------------------------------\n",
      "\n",
      "質問: When was the first GPT model released?\n",
      "回答: 2018\n",
      "確信度: 0.978\n",
      "--------------------------------------------------\n",
      "\n",
      "質問: How many parameters does GPT-3 have?\n",
      "回答: 175 billion\n",
      "確信度: 0.799\n",
      "--------------------------------------------------\n",
      "\n",
      "質問: What tasks can GPT-3 perform?\n",
      "回答: translation, \n",
      "question-answering, and code generation\n",
      "確信度: 0.942\n",
      "--------------------------------------------------\n",
      "\n",
      "質問: What architecture do these models use?\n",
      "回答: transformer\n",
      "確信度: 0.678\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 質問応答用のパイプラインを準備\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# コンテキスト（文脈となるテキスト）\n",
    "context = \"\"\"\n",
    "OpenAI's GPT (Generative Pre-trained Transformer) models have revolutionized natural language processing. \n",
    "The first GPT model was released in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020. \n",
    "GPT-3 contains 175 billion parameters and can perform various tasks like translation, \n",
    "question-answering, and code generation. In 2022, GPT-4 was announced, showing significant \n",
    "improvements in reasoning and creativity. These models are trained using transformer \n",
    "architecture and learn from vast amounts of internet text data.\n",
    "\"\"\"\n",
    "\n",
    "# テストする質問のリスト\n",
    "questions = [\n",
    "    \"When was the first GPT model released?\",\n",
    "    \"How many parameters does GPT-3 have?\",\n",
    "    \"What tasks can GPT-3 perform?\",\n",
    "    \"What architecture do these models use?\"\n",
    "]\n",
    "\n",
    "# 各質問に対して回答を生成\n",
    "print(\"質問応答テスト：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    print(f\"\\n質問: {question}\")\n",
    "    print(f\"回答: {result['answer']}\")\n",
    "    print(f\"確信度: {result['score']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
